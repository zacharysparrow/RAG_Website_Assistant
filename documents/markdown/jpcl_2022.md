      Image /page/0/Picture/0 description: The image shows the logo for "The Journal of Physical Chemistry Letters". The words "The Journal Of" are on the top line, "Physical Chemistry" is on the second line, and "Letters" is on the third line in a teal color. The words "A Journal of the American Chemical Society" are on the fourth line in a smaller font.

# Uniting Nonempirical and Empirical Density Functional Approximation Strategies Using Constraint-Based Regularization

Zachary M. Sparrow, Brian G. Ernst, Trine K. Quady, and Robert A. DiStasio, Jr.\*

Cite This: J. Phys. Chem. Lett. 2022, 13, 6896–6904

Image /page/0/Picture/6 description: The image shows a blue button with a white globe icon on the left and the text "Read Online" on the right.

## **ABSTRACT:** 
In this work, we present a general framework that unites the two primary strategies for constructing density functional approximations (DFAs): nonempirical (NE) constraint satisfaction and empirical (E) data-driven optimization. The proposed method employs B-splines, bell-shaped spline functions with compact support, to construct each inhomogeneity correction factor (ICF). This choice offers several distinct advantages over traditional polynomial expansions by enabling explicit enforcement of linear and nonlinear constraints as well as ICF smoothness using Tikhonov and penalized B-splines (P-splines) regularization. As proof-of-concept, we use the so-called CASE (constrained and smoothed empirical) framework to construct a constraint-satisfying and data-driven global hybrid that exhibits enhanced performance across a diverse set of chemical properties. We argue that the CASE approach can be used to generate DFAs that maintain the physical rigor and transferability of NE-DFAs while leveraging high-quality quantum-mechanical data to remove the arbitrariness of ansatz selection and improve performance.

Image /page/0/Figure/9 description: This figure shows a plot of F(u) vs u. The y-axis is labeled F(u) and ranges from 0.0 to 2.0. The x-axis is labeled u and ranges from 0.0 to 1.0. The plot shows several curves and data points. The top of the plot is labeled "Lieb-Oxford Bound" and the bottom is labeled "Negativity/Non-Positivity". There are two horizontal lines at the top and bottom of the plot, and a dashed line that goes from the top left to the bottom right. There are also several bell-shaped curves in the bottom half of the plot. The plot also shows data points labeled "Fixed gradient responses" and "Rapidly varying density limit". There is also an image of a person holding a bucket labeled "data".

## INTRODUCTION

Kohn–Sham density functional theory (KS-DFT) has Κ become the *de facto* standard for electronic structure calculations in chemistry, physics, and materials science due to its favorable trade-off between accuracy and computational cost.<sup>1</sup> While there now exist hundreds of density functional approximations (DFAs) of varying complexity across all rungs of Perdew's popular Jacob's ladder,<sup>2</sup> most have been designed using either nonempirical (NE) or empirical (E) strategies.<sup>1,3,4</sup> NE-DFA strategies construct DFAs by proposing simple ansätze designed to satisfy well-defined physical constraints and norms (e.g., the uniform electron gas (UEG) limit,<sup>5</sup> second-order gradient responses,<sup>6–9</sup> Lieb–Öxford bound<sup>10,11</sup>). Resulting NE-DFAs (e.g., Perdew-Burke-Ernzerhof (PBE),<sup>4</sup> PBE0,<sup>12</sup> SCAN<sup>13</sup> (strongly constrained and appropriately normed)) tend to be more transferable across complex condensed-phase systems, making them more favored in the physics and materials science communities. E-DFA strategies construct DFAs by optimizing a physically motivated and flexible functional form to best reproduce reference quantum-chemical data. The resulting E-DFAs (e.g., Becke, three-parameter, Lee-Yang-Parr (B3LYP),<sup>14</sup> Minnesota functionals,<sup>15–17</sup> B97 family<sup>1,3,18</sup>) often perform quite well (typically exceeding NE-DFAs) on chemical systems and properties similar to the training data, and tend to be more popular for chemical applications.

When used independently, both of these strategies have shortcomings. For one, NE-DFA ansätze are somewhat arbitrary, and there is some flexibility when constructing a NE-DFA that satisfies a given set of constraints;<sup>13,19</sup> hence, there is no guarantee that the chosen ansatz will perform best in practice. The choice of constraints is also somewhat arbitrary or empirical;<sup>20</sup> for example, the correct series expansion of the exchange-correlation energy  $(E_{xc})$  is sometimes ignored as it often results in inaccurate DFAs for real systems.<sup>21</sup> In the same breath, striving for the best-performing functional using an E-DFA strategy often goes hand-in-hand with sacrificing exact physical constraints, which is not ideal for transferability.<sup>3,15,18,22</sup> Furthermore, some E-DFAs suffer from nonphysical "bumps" or "wiggles" in the inhomogeneity correction factor (ICF), which violate an *implicit* smoothness constraint and can require significantly larger grids for accurate quadrature in practice.<sup>23–26</sup>

While both strategies provide useful information about the optimally performing DFA, neither suffices on its own. Hence, several groups have advocated for combining these strategies,<sup>21,27</sup> although constraint satisfaction during the datadriven optimization process has remained difficult to date. To address the E-DFA smoothness problem, the Bayesian error estimation functional (BEEF)<sup>28–30</sup> and Minnesota<sup>31</sup> functionals have adopted an explicit smoothness penalty in the regression procedure with reasonable success; the resulting ICFs are smoother than previous generations, albeit not always completely devoid of spurious features. Furthermore, the recent MCML (multi-purpose, constrained, and machine learned) approach<sup>27</sup> has made efforts to combine NE-DFA and E-DFA strategies by algebraically enforcing three linear constraints <span id="page-1-0"></span><span id="page-1-0"></span>during the optimization process (expanding on an approach originally used by Truhlar and co-workers when constructing numerous Minnesota functionals<sup>17,32,33</sup>). While successful in enforcing the targeted constraints, the polynomial basis used in MCML (and all but a few<sup>34</sup> E-DFAs) prevents explicit enforcement of nonlinear constraints (such as inequalities) and makes satisfying any additional constraints nontrivial as each regression coefficient appears in every algebraic constraint.

Image /page/0/Picture/17 description: The image shows the logo for ACS Publications. The logo consists of a blue and yellow diamond shape on the left, followed by the text "ACS Publications" in blue.

In this work, we address this long-standing challenge of uniting NE-DFA and E-DFA strategies by presenting a general framework that seamlessly enables one to enforce exact physical constraints and ICF smoothness while simultaneously leveraging high-quality quantum-mechanical data during DFA construction. The proposed constrained and smoothed empirical (CASE) framework uses B-splines (i.e., compact bell-shaped piecewise functions<sup>35</sup>) during ICF construction, which allows for a tunable trade-off between ICF smoothness and flexibility via penalized B-splines (P-splines),<sup>36</sup> as well as explicit enforcement of both linear and nonlinear constraints via generalized Tikhonov regularization. As proof-of-concept, we use this framework to construct a global hybrid DFA that completely satisfies all but one constraint (and partially satisfies the remaining one) met by the PBE0 NE-DFA. When compared to PBE0 (and the popular B3LYP E-DFA), this CASE-generated DFA exhibits improved performance across a diverse set of chemical properties without sacrificing transferability or requiring large numerical quadrature grids. As such, we argue that the CASE framework can be used to construct nextgeneration DFAs that maintain the physical rigor and transferability of NE-DFAs while leveraging high-quality quantummechanical data to remove the arbitrariness of ansatz selection and improve performance.

### **Proof-of-Concept: Functional Form.** 
As a proof-ofconcept illustration of the CASE approach, we constructed and critically assessed a constraint-satisfying global hybrid generalized gradient approximation (GGA). The overall functional form for this CASE-generated DFA (hereafter referred to as CASE21) was chosen to be as simple as possible and was assembled from well-established ingredients used during DFA construction for over two decades now, that is, gradient corrections to the semilocal  $E_{\rm xc}$  components in conjunction with a set fraction (25%) of exact exchange  $(E_{xx})$ , as generally recommended for global hybrid GGAs.<sup>12,37</sup> Our choice to construct a hybrid GGA (instead of a more complicated meta- or hybrid meta-GGA) was intentional, as the simpler functional form of GGA-based ICFs allows us to more clearly demonstrate how CASE can be used to enforce physical constraints and ICF smoothness during a data-driven DFA optimization procedure. However, the selection of such a relatively simple functional form is not a limitation of the CASE framework, which can be used to construct DFAs on every rung of Jacob's ladder. In fact, we expect that the full scope of this approach will be better realized when constructing more sophisticated functionals (e.g., meta- and hybrid meta-GGAs) that have the ability to satisfy more physical constraints and the flexibility to leverage larger amounts of benchmark data.

With these points in mind, we write CASE21 as the following sum of exchange and correlation contributions:

$$E_{\rm xc}^{\rm CASE21} = \frac{3}{4} E_{\rm x}[\rho_{\uparrow}, \rho_{\downarrow}] + \frac{1}{4} E_{\rm xx} + E_{\rm c}[\rho, \zeta]$$
(1)

The semilocal exchange is defined using the exchange spin scaling relationship:<sup>38</sup>

$$E_{\mathbf{x}}[\rho_{\uparrow}, \rho_{\downarrow}] = \frac{1}{2} (E_{\mathbf{x}}[2\rho_{\uparrow}] + E_{\mathbf{x}}[2\rho_{\downarrow}])$$
(2)

in which

$$E_{\mathbf{x}}[\rho_{\sigma}] = \int \rho_{\sigma} \epsilon_{\mathbf{x}}^{\text{LDA}}(\rho_{\sigma}) F_{\mathbf{x}}(u_{\mathbf{x},\sigma}) \, \mathrm{d}\mathbf{r}$$
(3)

 $\rho_{\sigma}$  is the spin density (with spin  $\sigma \in \{\uparrow,\downarrow\}$ ),  $c_x^{\text{LDA}}$  is the exchange energy density per particle within the local density approximation (LDA), and  $F_x(u_{x,\sigma})$  is the yet to be determined CASE21 exchange ICF. We employ  $0 \leq u_{x,\sigma} = (\gamma_x s_{\sigma}^2)/(1 + \gamma_x s_{\sigma}^2) < 1$  (as originally proposed by Becke<sup>39</sup>) as the finite-domain representation of the PBE dimensionless spin density gradient,  $s_{\sigma} = |\nabla \rho_{\sigma}|/[\pi^{2/3}(2\rho_{\sigma})^{4/3}]$ . Here, we note that the PBE exchange ICF can be written as a linear function of  $u_{x,\sigma}$  if  $\gamma_x = \mu/\kappa \approx 0.273022$  (where  $\mu$  and  $\kappa$  are the NE parameters in PBE), which we denote by  $\overline{F}_x(u_{x,\sigma}) \equiv 1 + \kappa u_{x,\sigma}$ . Hence, we argue that this is an appropriate choice for  $\gamma_x$  since the UEG exchange limit, <sup>5</sup> UEG linear response, <sup>4</sup> and Lieb–Oxford bound^{10,11} can still be straightforwardly enforced in this smooth limiting form (*vide infra*).

We construct  $E_{c}[\rho, \zeta]$  in analogy to  $E_{x}[\rho_{\sigma}]$ , namely,

$$E_{\rm c}[\rho,\,\zeta] = \int \rho \epsilon_{\rm c}^{\rm LDA}(\rho,\,\zeta) F_{\rm c}(u_{\rm c}) \,\,\mathrm{d}\mathbf{r} \tag{4}$$

in which  $\epsilon_c^{\text{LDA}}(\rho, \zeta)$  is the PW92<sup>40</sup> LDA correlation energy density per particle,  $\rho = \rho_{\uparrow} + \rho_{\downarrow}$  is the total density,  $\zeta = (\rho_{\uparrow} - \rho_{\downarrow})/\rho$  is the relative spin polarization, and  $F_c(u_c)$  is the yet to be determined CASE21 correlation ICF. As with exchange, we suggest a form for  $u_c$  such that a linear ICF, that is,  $\overline{F}_c(u_c) \equiv 1 - u_c$ , would satisfy the UEG correlation limit,<sup>5</sup> rapidly varying density limit,<sup>4</sup> and second-order gradient expansion for correlation.<sup>6-9</sup> Namely, we propose  $0 \le u_c \equiv (-\phi^3 t^2)/(-\phi^3 t^2 + \gamma_c \epsilon_c^{\text{LDA}}) < 1$ , where  $\phi = 1/2[(1 + \zeta)^{2/3} + (1 - \zeta)^{2/3}]$  is a spin scaling factor,<sup>6</sup>  $\gamma_c = 1/\beta \approx 14.986886$  (where  $\beta$  is another NE parameter in PBE), and t is the following dimensionless spinseparated density gradient:

$$t \equiv \sqrt{a_0} \left(\frac{\pi}{3}\right)^{1/6} \frac{|\nabla \rho_{\uparrow}| + |\nabla \rho_{\downarrow}|}{4\rho^{7/6} \phi} \tag{5}$$

This quantity reduces to the PBE dimensionless density gradient  $(t^{\text{PBE}}, \text{ which has } |\nabla \rho| \text{ instead of } |\nabla \rho_{\uparrow}| + |\nabla \rho_{\downarrow}| \text{ in the numerator})$ when  $|\nabla \zeta| = 0$ , which was assumed during the construction of PBE correlation and is a relationship that allows DFAs based on t to satisfy PBE correlation constraints. We note in passing that the use of  $t^{\text{PBE}}$  yields qualitatively similar results to t (which might be expected, given that t and  $t^{PBE}$  are equivalent for closedshell systems), although t slightly outperforms  $t^{PBE}$  quantitatively. With the above definition of  $u_c$ , eq 4 does not fully satisfy uniform scaling to the high-density limit for correlation;<sup>41</sup> however, it does completely cancel the  $\epsilon_{\rm c}^{\rm LDA}$  logarithmic singularity<sup>42</sup> and allows for satisfaction of all other PBE correlation constraints. Since the functional form described above was chosen for its simplicity, partial satisfaction of this constraint is not a restriction of the CASE approach; in principle, a functional form (albeit more complex) that completely satisfies all PBE correlation constraints could have been used.

### **The CASE Framework.** 
CASE exchange and correlation ICFs are written as linear combinations of  $N_{sp}$  compact piecewise bell-shaped cubic (k = 3) uniform B-spline basis functions ( $\{B_i\}$ ),<sup>35</sup> that is,

$$F_{\rm x}(u_{\rm x,\sigma}) &= \sum_{i}^{N_{\rm sp}} c_{{\rm x},i} B_i(u_{\rm x, \sigma}) = \bm{c}_{\rm x} \cdot \bm{B}_{\rm x,\sigma} $$
$$F_{\rm c}(u_{\rm c}) &= \sum_i^{N_{\rm sp}} c_{\rm c,i} B_i(u_{\rm c}) = \bm{c}_{\rm c} \cdot \bm{B}_{\rm c} ,$$(6)

<span id="page-2-0"></span><span id="page-2-0"></span>which is equivalent to constructing each ICF using a cubic spline<sup>43</sup> (see Supporting Information (SI) for more details). With an appropriate uniformly spaced knot vector,<sup>35,36</sup> the  $B_i(u_{x,\sigma})$  and  $B_i(u_c)$  are also uniformly spaced with all points in 0  $\leq u_{x,\sigma} \leq 1$  and  $0 \leq u_c \leq 1$  supported by three nonzero B-splines. As depicted in Figure 1a, setting  $c_x = 1 = c_c$  in eq 6 results in  $F_x(u_{x,\sigma}) = 1$  (LSDA exchange) and  $F_c(u_c) = 1$  (LDA correlation).

Image /page/2/Figure/3 description: The image shows two graphs, labeled (a) and (b), depicting B-spline basis functions. Both graphs have 'u' on the x-axis and 'F(u)' on the y-axis. Graph (a) displays a black curve that peaks at 1.0, with several colored curves beneath it, labeled B1 through B10. The coefficients c1 through c10 are all equal to 1. Graph (b) also features a black curve, but the colored curves beneath it have varying peaks, and the coefficients c1 through c10 are marked with dots on the black curve. The y-axis ranges from 0.0 to 1.0 in both graphs, and the x-axis ranges from approximately -0.2 to 1.2.

**Figure 1.** (a) B-spline basis functions  $({B_i}_{i=1,10}, \text{ rainbow})$  used to represent exchange and correlation ICFs in the CASE approach. When all expansion coefficients are set to unity, the B-spline curve (F(u) =  $\sum_{i} c_i B_i(u)$ , black) is uniform in  $0 \le u \le 1$  and recovers the LSDA/LDA limit. (b) B-spline curve with nonuniform coefficients. Note how the coefficients again closely align with the curve for  $0 \le u \le 1$ .

To seamlessly unite the NE-DFA and E-DFA strategies, the CASE approach uses generalized Tikhonov regularization<sup>44</sup> to determine  $\mathbf{c} = (\mathbf{c}_x, \mathbf{c}_c)$ , that is, ICF coefficients are found by minimizing the following loss function:

$$\mathcal{L} = \left\| \mathbf{X}\mathbf{c} - \mathbf{y} \right\|_{\mathbf{W}}^{2} + \lambda \left\| \mathbf{c} \right\|_{\mathbf{A}}^{2} + \eta \sum_{i} \left\| \mathbf{c} - \mathbf{c}_{0} \right\|_{\mathbf{Q}_{i}}^{2}$$
(7)

wherein  $\|\mathbf{v}\|_{\mathbf{M}}^2 = \mathbf{v}^{\mathrm{T}}\mathbf{M}\mathbf{v}$  is the matrix norm of the vector  $\mathbf{v}$  using the matrix M, the sum is over the enforced constraints, and all other quantities will be defined below. Hence, the key to determining c lies in appropriate matrix norm choices in each term in  $\mathcal{L}$ .

### Goodness of Fit. 
In the goodness of fit term (i.e., the first term in  $\mathcal{L}$ ), we construct the design matrix **X** by first noting that substitution of eq 6 into eqs 3 and 4 (with fixed orbitals) casts  $E_{\rm x}[\rho_{\sigma}]$  and  $E_{\rm c}[\rho, \zeta]$  into linear forms in  ${\bf c}_{\rm x}$  and  ${\bf c}_{\rm c}$ :

$$E_{\mathbf{x}}[\rho_{\sigma}] = \sum_{i}^{N_{sp}} c_{\mathbf{x},i} \int \rho_{\sigma} \epsilon_{\mathbf{x}}^{\mathrm{LDA}}(\rho_{\sigma}) B_{i}(u_{\mathbf{x},\sigma}) \, \mathrm{d}\mathbf{r} \equiv \mathbf{c}_{\mathbf{x}} \cdot \boldsymbol{\xi}_{\mathbf{x},\sigma}$$
$$E_{c}[\rho, \zeta] = \sum_{i}^{N_{sp}} c_{c,i} \int \rho \epsilon_{c}^{\mathrm{LDA}}(\rho, \zeta) B_{i}(u_{c}) \, \mathrm{d}\mathbf{r} \equiv \mathbf{c}_{c} \cdot \boldsymbol{\xi}_{c}$$
(8)

Hence, linear combinations of  $\xi_{x,\sigma}$  and  $\xi_{c}$  can be used to construct semilocal xc contributions to energy differences  $\Delta E_{\rm xc}$ (e.g., atomization energies, reaction energies, barrier heights) in a form amenable to linear regression using reference data. That is, defining  $\boldsymbol{\xi} \equiv (\boldsymbol{\xi}_x, \boldsymbol{\xi}_c)$ , with  $\boldsymbol{\xi}_x$  obtained after applying eqs 1 and 2 to  $\boldsymbol{\xi}_{x,\uparrow}$  and  $\boldsymbol{\xi}_{x,\downarrow}$  in eq 8, allows us to define **x** (a single row of **X**) as

$$\Delta E_{\rm xc} = \sum_{j} \nu_j (\mathbf{c} \cdot \boldsymbol{\xi}_j) = \mathbf{c} \cdot \sum_{j} \nu_j \boldsymbol{\xi}_j \equiv \mathbf{c} \cdot \mathbf{x}$$
(9)

in which  $\nu_i$  is the stoichiometric coefficient for the *j*-th component in  $\Delta E_{xc}$  (i.e., the energy of a molecule or atom), y is the corresponding vector of reference energy differences  $\Delta E_{xc}^{ref}$ and our choice for W (a square diagonal matrix of weights  $w_i \equiv$  $\min[1, 1/\Delta E_{xc,i}^{ref}])$  is motivated by the fact that the c minimizing the goodness of fit term only (i.e., weighted least-squares) is the best linear unbiased estimator (under some common assumptions) if the  $w_i$  are inversely proportional to the variance in each measurement.<sup>45</sup> Since  $E_{xc}$  is the only nonexact term in KS-DFT, both bias- and variance-type DFA errors should scale linearly with  $E_{xc}$ <sup>46-48</sup> making this a natural choice for W. Here, we argue that the piecewise nature of the B-spline curves used in CASE offers more flexibility than the low-order polynomial expansions often used to represent E-DFA ICFs (e.g., the B97 family  $^{1,3,18}$ ); with the ability to conform to more subtle shapes, a B-spline ICF should be able to better leverage the reference data.

### **ICF Smoothness.** 
For the second term in  $\mathcal{L}_{i}$  we note that Bsplines can be regularized by explicitly penalizing deviations from smoothness (i.e., ICF "bumps" or "wiggles") using Psplines, a regularization technique suggested by Eilers and Marx<sup>35,36</sup> based on the observation that B-spline coefficients closely resemble the B-spline curve (see Figure 1b). As such, ICF smoothness can be explicitly enforced in the CASE framework via a finite-difference penalty on c; in this work, we interpret nonsmoothness as nonlinearity in the ICF, and construct A from the second-derivative finite-difference matrix (see SI).  $\lambda$  is a hyperparameter that governs the relative importance of the smoothness and goodness of fit contributions to  $\mathcal{L}$ , and interpolates (assuming  $\eta \gg 1$ , vide infra) between linear ICFs (i.e.,  $\overline{F}_{x}(u_{x,\sigma})$  and  $\overline{F}_{c}(u_{c})$ ) that are completely constraint-driven  $(\lambda \to \infty)$  and wiggly ICFs that are data-driven to the *maximum* amount possible in this framework ( $\lambda \rightarrow 0$ ). As such, any nonlinearity in the final optimized ICFs can be attributed to the data. Here, we note that alternative interpretations of smoothness would result in penalizing other derivatives (e.g., F''(u)). Separately penalizing the exchange and correlation ICFs (i.e., using two  $\lambda$ -hyperparameters) is also possible if the ICF smoothness contributions to  $\mathcal{L}$  from  $F_x(u_{x,\sigma})$  and  $F_c(u_c)$  strongly differ. In this work, we found that P-spline regularization yields ICFs devoid of any spurious "wiggles" via single- $\lambda$  penalization of F''(u) (vide infra). In contrast, an excessively large penalty (which results in decreased performance) is usually required to remove all nonphysical "bumps" or "wiggles" in polynomial ICFs regularized via Tikhonov (or ridge) regression.<sup>28,49</sup> Furthermore, although such polynomial-based smoothness penalties are somewhat effective in reducing DFA grid dependence,<sup>24,31</sup> these approaches have been largely ineffective when enforced <span id="page-3-0"></span><span id="page-3-0"></span>alongside constraints.<sup>28,50</sup> On the other hand, we find no issues when simultaneously enforcing ICF smoothness in conjunction with numerous linear and nonlinear constraints using the CASE approach.

### Constraint Satisfaction. 
In the constraint satisfaction term in  $\mathcal{L}_i$  the  $\{\mathbf{Q}_i\}$  are chosen to measure constraint-specific deviations of c from  $c_0$ , the coefficients corresponding to  $F_x(u_{x,\sigma})$ and  $\overline{F}_{c}(u_{c})$ . Each  $\mathbf{Q}_{i}$  corresponds to a constraint on F(u) or F'(u) and is constructed such that any constraint-satisfying c yields  $\|\mathbf{c} - \mathbf{c}_0\|_{\mathbf{Q}_i}^2 = 0$  (see SI for  $\mathbf{Q}_i$  construction details).  $\eta$  is a hyperparameter that governs the relative importance of the constraint satisfaction contribution to  $\mathcal{L}_{i}$  and should be chosen to be large enough for strict constraint satisfaction but small enough to avoid conditioning issues. Since each B-spline has compact support, each  $\mathbf{Q}_i$  only enforces a constraint on a small subset of c (e.g., those corresponding to the nonzero B-splines at u = 0; in contrast, each constraint would generally involve every parameter in a polynomial-based ICF (e.g., MCML<sup>27</sup>). Another important consequence of this local support is that the B-spline curve itself will lie within the range of c (cf. Figure 1b). Hence, inequality constraints can be enforced via an iterative update to the corresponding  $\mathbf{Q}_i$  using the shape constraint algorithm (SCA) of Bollaerts et al.,<sup>51</sup> which fixes all inequality-violating  $c_i$ to the constraint boundary. In contrast, there is no straightforward way to apply inequality constraints on a polynomial-based ICF, as each basis function is uniquely shaped and has global support.

### **CASE21: Training Procedure.** 
Now we will demonstrate how the CASE framework described above can be used to train a fully self-consistent DFA, that is, the proof-of-concept CASE21 functional. Our self-consistent training procedure (Scheme 1, see Computational Methods for more details) leverages three distinct data sets: training ( $X_{train}$ ,  $y_{train}$ ), validation ( $X_{val}$ ,  $y_{val}$ ), and testing ( $X_{testr}$ ,  $y_{test}$ ). For CASE21, we fully enforce the following 10 physical constraints satisfied by PBE: exchange spin scaling,<sup>38</sup> uniform density scaling for exchange,<sup>52</sup> UEG exchange limit,<sup>5</sup> UEG linear response,<sup>4</sup> Lieb–Oxford bound,<sup>10,11</sup> exchange energy negativity, UEG correlation limit,<sup>5</sup> second-order gradient expansion for correlation,<sup>6–9</sup> rapidly varying density limit for correlation,<sup>4</sup> and correlation energy nonpositivity.<sup>4</sup> We also partially enforce uniform scaling to the high-density limit for correlation (*vide supra*).<sup>41,42</sup> In a given iteration, the training set (a single database of heavy atom transfer reaction energies, HAT707<sup>1,53</sup>) is used to initially determine **c** by minimizing  $\mathcal{L}$ (in conjunction with the SCA for satisfying inequality constraints) for a range of  $\lambda$  and a given set of orbitals { $\psi_i$ } (with initial { $\psi_i^0$ } generated using  $\overline{F}_x(u_{x,\sigma})$  and  $\overline{F}_c(u_c)$ ). With **c**( $\lambda$ ), a weighted root-mean-square error,

## Scheme 1. Self-Consistent Training Procedure for Generating DFAs in the CASE Framework

Image /page/3/Figure/5 description: The image is a flowchart outlining a process titled "Generate Initial Orbitals." It begins with input data Xtrain, Ytrain, and {Qi}, which leads to a "Minimize" step involving L(c; {λ}) as per Eq. (7). This step is connected to another input Xval, Yval, which then goes to an "Optimize wRMSE(λ)" step as per Eq. (10). The output of this optimization is λ\* = argmin wRMSE(λ) over λ, which is then combined with Xtrain ∪ Xval, Ytrain ∪ Yval, and {Qi} to feed into another "Minimize" step involving L(c; λ\*) as per Eq. (7). This step leads to a decision point labeled "c\* Converged?" If the answer is "no," it loops back to "Generate New Orbitals" to produce c\* => {ψi}, which then feeds back into the initial "Minimize" step. If the answer is "yes," it proceeds to "Final Testing" with input data Xtest, Ytest.

<sup>†</sup>Subject to inequality constraints enforced by the SCA.

wRMSE(
$$\lambda$$
) =  $\sqrt{\text{diag}(\mathbf{W}) \cdot \mathbf{r}(\lambda)^2 / \text{Tr}(\mathbf{W})}$  (10)

in which  $\mathbf{r}(\lambda) = \mathbf{X}_{val}\mathbf{c}(\lambda) - \mathbf{y}_{val}$  is the error vector and  $\mathbf{r}(\lambda)^2$  is the element-wise square of  $r(\lambda)$ , is computed on the validation set (which contains absolute energies of H–O from AE18<sup>1,54</sup> and all atomization energies in TAE203<sup>55,56</sup>). Using  $\lambda^* = \operatorname{argmin}_{\lambda}$ wRMSE( $\lambda$ ), c\* is determined by reoptimizing  $\mathcal{L}$  (in conjunction with the SCA) over the training and validation sets. New  $\{\psi_i\}$  are then generated using  $c^*$ , and the entire cycle is repeated until c\* is stationary. At this point, the testing set (which contains more diverse chemical properties than the training and validation sets, vide infra) is used to assess performance and transferability. During initial minimization of eq 7, we found that  $c^*$  was fairly insensitive to the choice of training data, and the determination of  $\lambda^*$  was most robust if the training and validation sets contained distinct chemical properties. Hence, we limited the databases used for the training and validation sets to only a few chemical properties (i.e., reaction, atomization, and absolute energies), as this emphasizes transferability when evaluating the more diverse set of properties in the testing set. In particular, HAT707,<sup>1,53</sup> TAE203,<sup>55,56</sup> and AE18<sup>1,54</sup> were chosen because they are fairly large and reliable databases comprised of distinct chemical properties that collectively quantify the energies of covalent bonds (HAT707) relative to the energies of atoms (TAE203 and AE18).

We used  $N_{sp} = 10$  in Scheme 1 to generate the self-consistently optimized CASE21 DFA (six iterations; convergence criterion of  $|\Delta \mathbf{c}| < 10^{-5}$ ; see SI for  $\mathbf{c}^*$ ). Even with a finite  $\eta$  value ( $\eta = 10^8$ ), CASE21 nearly exactly satisfies all enforced constraints, that is,  $F_{x}(0), F_{c}(0), F'_{x}(0)$ , and  $F'_{c}(0)$  differ from their corresponding exact values by  $\sim 10^{-5}$ ,  $F_c(1)$  differs by  $\sim 10^{-6}$ , and all other constraints are exactly satisfied. When nonzero, the deviations were similar in magnitude to the convergence criterion and negligible in practical calculations. We therefore conclude that the proposed CASE framework successfully enforced all constraints without sacrificing smoothness, which still remains a challenge for other DFA training procedures.<sup>28,50</sup> To confirm that CASE21 remains representative of DFAs trained with other  $N_{\rm sp}$  values (and to investigate the dependence of the CASE framework on  $N_{sp}$ ), we non-self-consistently optimized c for select  $N_{sp} \in [6,40]$  using the CASE21 orbitals. As depicted in Figure 2, the resulting ICFs and their first derivatives were all smooth and very similar (particularly for  $N_{sp} \ge 10$ ), and the number of *effective* degrees of freedom<sup>57</sup> (DoF, see SI for derivation and more details) change slowly for  $N_{sp} \ge 10$ . We therefore expect little dependence on  $N_{\rm sp}$  for any DFA constructed with 10-40 B-splines and use this observation as an *a posteriori* justification for our choice of  $N_{\rm sp}$  = 10. From the piecewise nature of F'(u) in this plot, one can also see that the CASE21 ICFs (DoF = 1.22) subtly deviate from linearity in ways that cannot be precisely obtained using low-order polynomial expansions. Here, we also note that the CASE21 xc enhancement factors (see SI) are noncrossing for different  $r_s$  values, which is a consequence of satisfying uniform density scaling for correlation at the GGA level;<sup>52</sup> although this additional constraint was not explicitly enforced during the construction of CASE21 or PBE, both of these DFAs have this property.

<span id="page-4-0"></span><span id="page-4-0"></span>Image /page/4/Figure/1 description: The image contains two plots. The top plot shows two functions, F(u) and F'(u), plotted against u. F(u) is represented by a blue line, and F'(u) is represented by a red line. The plot also includes the labels "Exchange" and "Correlation". The y-axis on the left is labeled "F(u)", and the y-axis on the right is labeled "F'(u)". The bottom plot shows the degrees of freedom (DoF) plotted against Nsp. The plot shows a series of data points that increase and then level off. The y-axis is labeled "DoF", and the x-axis is labeled "Nsp".

**Figure 2.** (top) Exchange and correlation ICFs (blue) and first derivatives (red) for select  $N_{sp} \in [6,40]$ . Highlighted curves (dark blue and dark red) correspond to the self-consistently optimized CASE21 DFA (with  $N_{sp} = 10$ ). Dashed lines represent the parameter-free linear ICFs ( $\overline{F}_x(u_{x,o})$  and  $\overline{F}_c(u_c)$ ) designed to satisfy the same constraints as CASE21. (bottom) Effective degrees of freedom (DoF) for select  $N_{sp} \in [6,40]$ , with the dark purple point corresponding to CASE21.

### CASE21: Final Testing. 
Having demonstrated that the CASE approach is able to enforce physical constraints and ICF smoothness in conjunction with a data-driven optimization procedure, we now compare the performance of CASE21 to the PBE0 and B3LYP hybrid DFAs across a diverse set of chemical properties in Figure 3. Since correlation is treated semilocally in CASE21, PBE0, and B3LYP, only databases containing small-tomedium molecules with minimal contributions from nonlocal correlation were selected for the testing (as well as training and validation) set; this also applies to the noncovalent interaction (NCI) databases used here, which are mostly comprised of hydrogen- and halogen-bonded dimers (instead of primarily dispersion-bound systems). In this work, we report the robust mean absolute error (MAE) metric for the selected DFAs on databases grouped by chemical property, as this avoids the use of arbitrarily weighted error metrics (which would otherwise be needed to account for the differences in frequency and magnitude of the various properties in the training, validation, and test sets).

With these points in mind, we find that CASE21 outperforms the PBE0 NE-DFA on 11 of 12 properties, with improvements as large as 0.81 and 0.82 kcal/mol for bond dissociation energies (BDE) and electron affinities (EA), respectively. In the testing set, CASE21 decreases the PBE0 MAE in 8 of 9 properties by an average of 0.34 kcal/mol. On the other hand, PBE0 outperforms CASE21 for ionization potentials (IP), which may be attributed to incomplete or partial satisfaction of uniform scaling to the high-density limit for correlation in CASE21,<sup>42</sup> as this results in slightly less accurate (but still reasonable) correlation energies in the He isoelectronic series (see SI for more details). CASE21 also outperforms B3LYP (a popular E-DFA for chemical applications) on 8 of 12 properties; in the testing set, CASE21 decreases the B3LYP MAE in 6 of 9 properties by an average of 0.41 kcal/mol (while B3LYP only offers a marginal ~0.05 kcal/ mol improvement on the remaining 3 of 9). In this assessment, we also measured the performance of CASE21 for molecular structure optimization using the geometry energy offset (GEO) metric of Vukovic and Burke,<sup>88</sup> and found that CASE21 improves upon PBE0 and performs on par with B3LYP, which is recognized as one of the best DFAs for predicting molecular geometries. To put these results into context, the more advanced  $\omega$ B97X ( $\omega$ B97X-V) E-DFA<sup>18,91</sup> improves upon the PBE0 MAE for 8 of 9 (9 of 9) properties in the testing set by an average of 0.58 kcal/mol (0.59 kcal/mol). Despite the fact that CASE21 is a hybrid GGA that does not include range-separated exact exchange and nonlocal correlation, the improved performance of CASE21 in the testing data set is 58% (57%) on average of that achieved by  $\omega$ B97X ( $\omega$ B97X-V). Encouraged by these results, we also considered how the performance of CASE21 depends on the fraction of exact exchange, but ultimately found that the initial value of 1/4 was essentially optimal (see SI for more details). Although our focus to this point has been on molecular properties, CASE21 calculations of the lattice constants, bulk moduli, and cohesive energies of bulk Si and C (diamond) also showed promising preliminary results for solidstate properties (see SI). For these systems, the CASE21 predictions were significantly better than B3LYP and slightly worse than PBE0, suggesting that further studies into the performance of CASE21 on solid-state properties (as well as the inclusion of such properties in the training and validation sets) are warranted. Taken together, this analysis demonstrates that CASE21 is largely able to preserve the physical rigor and transferability of the PBE0 NE-DFA while offering a noteworthy increase in performance on chemical systems (even when compared to the B3LYP E-DFA), despite having only 1.22

Image /page/4/Figure/8 description: The image is a bar graph comparing the Mean Absolute Error (MAE) in kcal/mol for different computational methods (CASE21, PBE0, and B3LYP) across various chemical properties. The x-axis lists the properties: HAT (707), TAE (203), AE (8), RXN (111), BDE (109), PA (39), BH (103), IP (59), EA (38), IE (775), NCI (111), and GEO\* (69), with the numbers in parentheses indicating the number of data points for each property. The y-axis represents the MAE in kcal/mol, ranging from 0.0 to 6.0. The graph shows the MAE values for each method and property, with CASE21 represented by blue bars, PBE0 by black bars, and B3LYP by orange bars. Numerical values are displayed above some of the bars, such as -0.62 above the HAT bar for CASE21, -0.79 above the TAE bar for CASE21, -11.14 above the AE bar for CASE21, -0.51 above the RXN bar for CASE21, -0.81 above the BDE bar for CASE21, -0.05 above the PA bar for CASE21, -0.29 above the BH bar for CASE21, -0.43 above the IP bar for CASE21, -0.82 above the EA bar for CASE21, -0.06 above the IE bar for CASE21, -0.12 above the NCI bar for CASE21, and -0.34 above the GEO\* bar for CASE21.

**Figure 3.** Mean absolute errors of CASE21 (blue), PBE0 (gray), and B3LYP (orange) in the training/validation (shaded region) and testing (white region) sets. Bar labels indicate the relative performance of CASE21 and PBE0, with green and red numbers representing increased (MAE<sup>CASE21</sup> < MAE<sup>PBE0</sup>) and decreased (MAE<sup>CASE21</sup> > MAE<sup>PBE0</sup>) performance, respectively. Properties (number of data points) include HAT (heavy atom transfer reaction energies),<sup>1,53</sup> TAE (total atomization energies),<sup>55,56</sup> AE (absolute energies),<sup>1,54</sup> RXN (reaction energies),<sup>1,53,56,58-64</sup> BDE (bond dissociation energies),<sup>1,53,56,56,61,65</sup> PA (proton affinities),<sup>56,61,66,67</sup> BH (barrier heights),<sup>1,56,58,61,65</sup> PA (proton affinities),<sup>1,51,56,58,61,68,69</sup> EA (electron affinities),<sup>1,58,68,69</sup> IE (isomerization energies),<sup>1,1,15,16,53,56,58,61,67,70-80</sup> NCI (noncovalent interaction energies),<sup>1,1,81-87</sup> and GEO (geometry energy offsets).<sup>88-90</sup> See SI for more details regarding the databases used in this work. The asterisk (\*) indicates that all three GEO MAEs were scaled by 10× for clarity.

<span id="page-5-0"></span><span id="page-5-0"></span>effective DoF (compared to the 3.0 effective DoF in B3LYP; see SI for derivation).

Although the CASE21 ICFs are clearly smooth (cf. Figure 2), we also investigated the grid dependence of this DFA for completeness. Since Lebedev–Treutler grids<sup>92</sup> with 50 radial and 194 angular grid points (i.e., (50, 194)) are typically large enough to obtain accurate energetics with standard hybrid GGAs (such as PBE0),<sup>23</sup> we compared the performance of CASE21 using this grid to the larger grids employed during the training procedure (see Computational Methods). Using all points in the training, validation, and testing data sets (N = 2263; excluding GEO), we found nearly identical mean absolute deviations of  $1.84 \times 10^{-2}$  kcal/mol for CASE21 and  $1.83 \times 10^{-2}$  kcal/mol for PBE0, thereby indicating that CASE21 does not require larger quadrature grids than PBE0 for accurate integration and demonstrating the effectiveness of the P-spline regularization central to the CASE framework.

### Closing Remarks. 
In this work, we presented the CASE (constrained and smoothed empirical) framework for uniting the NE-DFA and E-DFA construction paradigms. By employing a B-spline representation for the ICFs, this approach has several distinct advantages over the historical choice of a polynomial basis, namely, explicit enforcement of linear and nonlinear constraints (using Tikhonov regularization) as well as penalization of nonphysical ICF "bumps" or "wiggles" (using Psplines), that are seamlessly integrated with data-driven optimization. As proof-of-concept, we used this approach to construct a global hybrid GGA that completely satisfies all but one constraint (and partially satisfies the remaining one) met by the PBE0 NE-DFA. Despite being trained on only a handful of properties, this CASE-generated DFA outperforms PBE0 and B3LYP (arguably the most popular E-DFA for chemical applications) across a diverse set of chemical properties. As such, we argue that the CASE framework solves the longstanding problem of uniting these seemingly disparate DFA strategies, and can be used to design next-generation DFAs that maintain the physical rigor and transferability of NE-DFAs while leveraging benchmark quantum-mechanical data to remove the arbitrariness of ansatz selection and improve performance. Alternatively, the CASE framework can also be used to enforce ICF smoothness in conjunction with physical constraints during NE-DFA construction (i.e., without leveraging data) or enforce ICF smoothness in conjunction with data-driven optimization during E-DFA construction (i.e., without requiring constraint satisfaction). Future work will extend this approach to more sophisticated DFAs (e.g., meta-GGAs, range-separated hybrids, DFAs with nonlocal correlation) that have the ability to satisfy more physical constraints and the flexibility to leverage larger amounts of data, where we expect that the larger function space made accessible by a B-spline ICF expansion will provide even more significant advantages over traditional low-order polynomials. Future work will also explore the performance of CASE-generated DFAs when treating condensed-phase systems as well as the use of B-splines for constructing robust features for machine-learning chemical properties.

### **Computational Methods.** 
All gas-phase electronic structure calculations were performed using in-house versions of Psi4 (v1.3.2)<sup>93</sup> and LibXC (v4.3.4)<sup>94</sup> modified with a self-consistent implementation of the CASE21 DFA (including functional derivatives analytically computed using Mathematica v12.1). Self-consistent field (SCF) calculations were performed using density fitting (DF) in conjunction with the def2-QZVPPD<sup>95,96</sup> and def2-QZVPP-JKFIT<sup>97,98</sup> basis sets and an energy convergence threshold of e\_convergence = 1e-12. During DFA training, all calculations employed (99, 590) Lebedev-Treutler grids<sup>92</sup> except for the calculations of the absolute energies in AE18,<sup>1,54</sup> which used (500, 974). Minimization of  $\mathcal{L}$  in eq 7 and optimization of wRMSE( $\lambda$ ) in eq 10 were performed in Mathematica v12.1. All solid-state electronic structure calculations were performed using the PWscf package in Quantum ESPRES-SO,<sup>99</sup> in conjunction with norm-conserving HSCV-PBE pseudopotentials<sup>100,101</sup> and converged planewave kinetic energy cutoffs (40 Ry and 120 Ry for Si and C, respectively) and k-point grids (4 × 4 × 4 and 8 × 8 × 8 for Si and C, respectively). All solid-state properties were determined by fitting the Murnaghan equation of state<sup>102</sup> to 10 points centered around the expected equilibrium lattice constant.

### ASSOCIATED CONTENT

### **Supporting Information**

The Supporting Information is available free of charge at https://pubs.acs.org/doi/10.1021/acs.jpclett.2c00643.

B-spline definitions; enforcement of ICF constraints; training, validation, and testing data sets; derivation of optimal coefficients and effective degrees of freedom for weighted generalized Tikhonov regularization; optimized CASE21 ICF coefficients; comparison of PBE and CASE21 xc enhancement factors; correlation energies in the helium isoelectronic series; assessment of the fraction of exact exchange in CASE21; and solid-state properties (PDF)

Reference/benchmark, CASE21, PBE0, and B3LYP energies for each energy difference in the training, validation, and testing data sets (XLSX)

### AUTHOR INFORMATION

### **Corresponding Author**

Robert A. DiStasio, Jr. – Department of Chemistry and Chemical Biology, Cornell University, Ithaca, New York 14853, United States; o orcid.org/0000-0003-2732-194X; Email: distasio@cornell.edu

### Authors

- Zachary M. Sparrow Department of Chemistry and Chemical Biology, Cornell University, Ithaca, New York 14853, United States; Ocrid.org/0000-0001-6163-2843
- Brian G. Ernst Department of Chemistry and Chemical Biology, Cornell University, Ithaca, New York 14853, United States; Orcid.org/0000-0002-7900-9360
- Trine K. Quady Department of Chemistry and Chemical Biology, Cornell University, Ithaca, New York 14853, United States; © orcid.org/0000-0001-7777-909X

Complete contact information is available at: https://pubs.acs.org/10.1021/acs.jpclett.2c00643

### Notes

The authors declare no competing financial interest.

### ACKNOWLEDGMENTS

All authors thank Kieron Burke, Garnet Chan, Alexandre Tkatchenko, and Don Truhlar for helpful scientific discussions and Susi Lehtola for implementing CASE21 in LibXC (v5.1.7). All authors also thank Richard Kang and Dzmitry (Dima) Vaido for their help in assembling databases and making <span id="page-6-0"></span><span id="page-6-0"></span>modifications to the Psi4 and LibXC codes, Hsin-Yu Ko for help in computing the solid-state properties, and Yang Yang for assisting with basis set selection for the He isoelectronic series. This material is based upon work supported by the National Science Foundation under Grant No. CHE-1945676. This work was supported in part by the Cornell Center for Materials Research with funding from the Research Experience for Undergraduates program (DMR-1757420 and DMR-1719875). R.A.D. also gratefully acknowledges financial support from an Alfred P. Sloan Research Fellowship. This research used resources of the National Energy Research Scientific Computing Center, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC02- 05CH11231.

# REFERENCES
References were removed from this document. Please see the full publication for details.
